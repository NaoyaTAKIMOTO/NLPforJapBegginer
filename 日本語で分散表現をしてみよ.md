
##目次
## 本文の目的について
## 自然言語処理について
## 日本語の自然言語処理について
### 計算機が言語を扱うということ
## W2Vの実装
## 参考文献


## 本文の目的について

このスクリプトは自然言語処理について紹介し、実際に実装を経験していただくためのものです。
---
みなさまの学び、技術や知識の裾野を広げるための一助になれば幸いです。
---

みなさま、スマホやPCをお持ちでしょうか？

ほとんどの方がお持ちだと思います。

そしてみなさま思い思いに活用されていることと思います。

そんなスマホですが、これから我々にさらなる恩恵をもたらそうとしています。

ご存知の方も多いかと思いますが、そう、AIです。

正直に申し上げればAIという言葉が宣伝文句の中で一人歩きしているきらいはありますが、
それに類する技術が飛躍的に進歩していることは事実です。

例えば**画像認識**、被写体の顔を認識してカメラがピントを合わせる、表情を認識してシャッターを切る、スマホのロックを解除する、被写体に応じて写真を自動で分類分けする、さらには入国管理にも用いられるなど実社会にも大きな影響力を持ってきております。

また自動運転や運転補助といった生活に密接に関わる自動車にもいくつか関連する機能がすでに搭載され始めています。

これらの技術はこれからさらなる発展を遂げて、みなさまの生活を一新してしまう時がくることでしょう。

これらの例は馴染みが深いものが多く、非常に想像がしやすいでしょう。


では**自然言語処理**についてはいかがでしょうか？

私は幸運にも仕事の関係で自然言語処理に携わっております。

そのため今回はこのもう一つの柱であるAIのジャンル、
自然言語処理についてみなさまに紹介させていただくとともに、
実際にその技術の一端に触れていただこうと考えています。


## 自然言語処理について

まずほとんどの方は**自然言語処理**という単語自体に見覚えがないことでしょう。

これは**自然言語**、つまり我々が日常的に用いている日本語、英語、その他人類が用いる言語を、

計算機で色々と**処理**することによって様々なことができるのではないか、というジャンルになっています。

ざっと挙げるだけでもその応用範囲は


1.   **音声認識**

        みなさまのスマホには音声入力機能があります。
        
        利用されたことのない方はぜひ一度お試しください。
        
2.   **各種検索**

        Googleやyahoo!, Bing などの検索サービスは何かしら利用されているのではないでしょうか？
        
        今日の技術書典にお越しになる際にGoogleマップを利用された方も多いのでは？
        
        アマゾンや楽天での商品検索やYouTubeで動画を検索するといったこともありますね。
3. **文書要約**

    長い文章なんて読みたくない！というのは何も勉強嫌いの学生に限った話ではありません。
    
    時間の貴重なビジネスパーソンにこそ需要があります。
    
    ニュースサイトの要約文などはもしかしたら既にAIが作成したものかもしれません。

4. **翻訳**

    英語の文書をGoogle翻訳やエキサイト翻訳で日本語に変換することはよくあることかと思います。
    
以上は簡単な例の紹介になりますが、

自然言語処理というジャンルを身近なものとして感じていただけたのではないでしょうか？

自然言語処理は大きく分けて
1. 音声を処理する分野(文字起こし、感情解析)

2. 文章から意味を汲み取って、さらに処理を行う(検索、翻訳、要約)

とに分かれます。
今回は後者に焦点を当てていきます。

ここでは文章の意味を汲み取り（分散表現の埋め込み）、
意味検索や意味空間における演算（有名なのは"King"-"Man"+"Woman"="Queen"）を行うところを目指します。

また便利なライブラリを紹介していきます。

一通りのタスクが終わった頃には、自然言語処理に一歩を踏み込むところまで進んでいるのではないでしょうか？

## 日本語の自然言語処理という問題について

研究も盛んに行われて、進歩も著しいAIというジャンル、
我々は将来その恩恵にあやかれるのか、と言いますと、

実は自然言語処理という分野においては確実にそうとは言い切れません。

いえむしろ世界の多数の人々はこのままではその飛躍から取り残されてしまうかもしれないのです。

と言いますのは、自然言語処理は主に英語を対象として研究が行われています。

そのためデータセット、研究環境、研究成果において英語はとても恵まれています。

多くの研究者も英語を対象にして活動を行なっています。

ではその何が問題なのか？

一番の問題は**英語の研究成果はそのままでは日本語に活用することができない**ということです。

つまり、どれだけ便利なサービスがアメリカで新しく実用化されても、日本人だけはそれが使えない、ということが起こり得ます。

いえいえ、これは英語以外の全ての言語において起こっている問題なのです。

みなさんご承知の通り、言語というのは日本語、英語、中国語、韓国語と比較的身近な言語でさえ、文字が違う、文法が違う、ニュアンスが違えば、熟語も違う、と様々なことが異なってくるのです。

もっと身近なもので具体的な例でいえば、同じ日本語の中でも方言において同様の問題が見られます。
関西弁ではモノを片付けることを**なおす**と言います。
「その麦茶、冷蔵庫に**なおし**といて」
といった使い方をします。
冷蔵庫壊れてるの？なんて思うかたも居るのではないでしょうか。

実際に言われた時にみなさまはすぐに理解できるでしょうか？
とてもすぐには対応できない、という方もいるでしょう。
そして、それが方言ではなく、異なる言語だとどうでしょう？
まして、人間でも苦労することが、計算機に簡単にできる道理があるでしょうか？

そういった問題の解決には多くの方の協力と努力が必要になります。




### 計算機が言語を扱うということ

もっとも機械にとって簡単な方法は文字ごとに一致するか否かを判別する全文検索という方法になります。
これは普段使っている多くの検索エンジンが採用している手法になります。

しかし、文字列や単語が一致するか否かだけを判断基準にしているというのは、実は人間の直感的な理解とはずれています。
というのもこの方法では同じ意味の単語でも表現が異なれば、検索にヒットしないということがあります。
スマホとスマートフォンでは同じものを意味しますが、全文検索では片方がヒットしても、もう片方がヒットしないということが起こり得ます。

ではどうすればこのような問題を解決できるのか。
その一つの解決法は二つの単語は同じものだという辞書を作成するというものです。
スマホが入力された時にはその裏でスマートフォンも同時に検索するという処理を行ってやれば意図した検索結果が得られるでしょう。

ですが問題もあります。
その辞書はどうやって作成するのでしょうか？
紙の辞書のように人がデータを一つづつ入力していけば良いのでしょうか？
それでは手間と時間がかかりすぎます。
では自動で辞書を作成するようにすればいいでしょうか？




### 単語の表現

基本的なアイデアは人間が外国語を翻訳しようとする場合に似ています。
一つ一つ単語の意味を調べて、文法に沿って対応関係を把握する。
これに尽きます。

しかし計算機はどうやって単語の意味を知ればいいのでしょうか？
辞書を入力すれば理解してくれるでしょうか？
いえいえそんなことはありません。
それができるのはあなたが自然言語をすでに習得しているからです。

計算機にとって一番かんたんな方法は単語ごとに番号をつけて区別するという方法です。
それだと単語の区別はついても、意味についての情報は何も持っていないということになりますよね。
一万単語あれば番号は一万番までつける必要があります。
さらにただの数字というのは扱いに困ります。
自然：１、言語：２、処理：３というように番号が振られている時に、自然と言語の二つの単語が同時に現れた場合にどのように表現できるでしょうか？
自然＋言語＝３という計算をすると処理を指すようになってしまうので、この数字の扱いは非常に難しい。

そこで単語をベクトルで表現できれば素敵だと思いませんか？
ベクトルで表現できれば演算が可能になります。足し算と引き算だけでもグッと世界が広がります。
素朴なアイデアは番号と語彙のサイズを用いてonehotベクトルにしてしまうことです。
しかし語彙が増えるに従って単語を表す番号は線形に膨れ上がっていきます。
これは語彙が増えるに従って、一つの単語が必要とするメモリ消費量が増えますし、語彙のサイズが異なる他の語彙を利用することができません。

これでもまだ足りない。もっと圧縮して情報を抽出したい・・・！
という方のために発明されたのがW2Vという手法になります。

それではW2Vを実際に作ってみようではありませんか！

## W2Vの実装

あまり文章ばかりが続いても退屈だと思いますので、ここから先はみなさまにも手を動かしていただこうと思います。
といっても実行可能な具体例を記載しています。
気になる点については修正や改造を加えて遊んでみてください。

Googleのアカウント(gmail, youtubeなど)をお持ちであれば、
無償で利用できるGoogle Colaboratoryというサービスを利用します。
これはpythonを利用するための環境とGPUを利用できる計算資源を無償で利用できるサービスです。
google drive上に~.ipynbを配置して、ファイルを右クリック、アプリケーションからgoogle colaboratoryを選択するだけで利用が可能です。

環境構築の手間や計算資源を提供してくれるGoogleに感謝を捧げます。



### グーグルドライブのマウント


まずはグーグルドライブに"Colab_data"というフォルダを作りましょう。

そのフォルダに日本語の.txtファイルを入れてください。

この文書ファイルはなんでも良いので、青空文庫やWikipediaから好きな文書を選んでも良いですし、ご自分で別に用意されても面白いでしょう。


そして以下のコマンドを実行しましょう。
認証用のリンクが出てくるので、リンク先からパスワードを取得して、以下に入力してください。


```
from google.colab import drive
drive.mount('/content/gdrive')
```

    Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code
    
    Enter your authorization code:
    ··········
    Mounted at /content/gdrive


これでcolab環境からグーグルドライブにアクセスができるようになります。

今回の教材では青空文庫より芥川龍之介著、蜘蛛の糸を対象にして進めていきます。

bashコマンドは行頭に!をつけることで利用可能です。
以下のコマンドで青空文庫からテキストをダウンロード、解凍、移動しています。


```
!wget https://www.aozora.gr.jp/cards/000879/files/92_ruby_164.zip
!unzip 92_ruby_164.zip
!mv kumono_ito.txt gdrive/My\ Drive/Colab_data/sample.txt
```

    --2019-09-03 03:01:47--  https://www.aozora.gr.jp/cards/000879/files/92_ruby_164.zip
    Resolving www.aozora.gr.jp (www.aozora.gr.jp)... 59.106.13.115
    Connecting to www.aozora.gr.jp (www.aozora.gr.jp)|59.106.13.115|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 3887 (3.8K) [application/zip]
    Saving to: ‘92_ruby_164.zip’
    
    92_ruby_164.zip     100%[===================>]   3.80K  --.-KB/s    in 0s      
    
    2019-09-03 03:01:48 (940 MB/s) - ‘92_ruby_164.zip’ saved [3887/3887]
    
    Archive:  92_ruby_164.zip
    Made with MacWinZipper™
      inflating: kumono_ito.txt          


### 単語への分割について

日本語の特徴の一つとして、全ての単語がスペースで区切られていないという問題があります。
そこでまずは文章を単語に分割していきましょう。

今回はpythnoから利用できるトークナイザとして扱いが簡単なjanomeというライブラリを利用します。
以下のコードを実行してjanomeをインストールします。


```
# install mecab
!pip install janome
from janome.tokenizer import Tokenizer

```

    Collecting janome
    [?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/560f4c9ff01a584b1ecd1da981e82d0077c079ecba84571b4f623680300e/Janome-0.3.9-py2.py3-none-any.whl (25.1MB)
    [K     |████████████████████████████████| 25.1MB 1.8MB/s 
    [?25hInstalling collected packages: janome
    Successfully installed janome-0.3.9


そして次のブロックを実行すると、文が単語に区切られて出力されます。
トークナイザを使って、文をトークナイズして、そのトークンの中から指定した品詞のもののみを返すようにしています。

今回は名詞と動詞のみ指定して抜き出しています。

もちろん好みで他の品詞を追加するのもオススメです。


```
tokenizer = Tokenizer()
def extract_words(text):
    """もし単語が動詞もしくは名詞なら,
     その単語の原型を返し、リストの形で返す
     """
    tokens = tokenizer.tokenize(text)
    return [token.base_form for token in tokens
            if token.part_of_speech.split(',')[0] in ['名詞', '動詞']]


sample_text = "例えばこんな風に適当な文があるとします。"

extract_words(sample_text)
```




    ['風', '適当', '文', 'ある', 'する']



### 単語の表現について

さて、文を単語に分割することはできるようになりました。

ですが、ちょっと待ってください。

この単語、どうやって扱いましょうか？

計算機に単語の意味を理解させるためには工夫が必要です。

今回はその工夫の中でも有名なもの、**Word2Vec**をいよいよ実装してもらいます。

これは**単語は前後の文章と似た意味を持っている**という前提を用いたものです。

いったいどういうことでしょうか？

猫という単語を例に考えてみましょう。
猫という動物を全く知らない地方の人が、猫について学習するとき、どのように考えるでしょうか？
試しにいくつか猫が出てくる小説を読ませてみましょう。
夏目漱石の有名な小説に"吾輩は猫である"というものがあります。その語り出しはご存知の方も多いでしょう。

**吾輩は猫である。名前はまだない。 どこで生れたか頓と見当がつかぬ。**

このとき、猫について何も知らない人はこう思うわけです。

- 猫は我輩という単語に関係があるんだな。
- 名前を持つものなのか。
- ほう、猫とは生まれるものなのか。

というように前後の文章から猫について学習するわけです。
この学習をたくさんすればきっと猫について詳しくなることでしょう。

同じことが計算機についても言えます。
計算機は猫について何も知りません。
いえいえ、猫はおろか日本語については何も知らないでしょう。
そこでたくさんの文書からいろいろな日本語の単語の意味を学習してもらうわけです。

### テキストの準備
まずはテキストを読み取ってもらいましょう。
ここではグーグルドライブのColab_dataというフォルダのsample.txtというファイルを指定しています。
パスは各自で修正してください。


```
text_path = "/content/gdrive/My Drive/Colab_data/sample.txt"#"sample.txt"は適宜変更してください。
with open(text_path,encoding="shift_jis") as f:
    novel = f.read()
print(novel)
```

    蜘蛛の糸
    芥川龍之介
    
    -------------------------------------------------------
    【テキスト中に現れる記号について】
    
    《》：ルビ
    （例）蓮池《はすいけ》のふち
    
    ｜：ルビの付く文字列の始まりを特定する記号
    （例）丁度｜地獄《じごく》の底に
    
    ［＃］：入力者注　主に外字の説明や、傍点の位置の指定
    　　　（数字は、JIS X 0213の面区点番号、または底本のページと行数）
    （例）※［＃「特のへん＋廴＋聿」、第3水準1-87-71］
    -------------------------------------------------------
    
    ［＃８字下げ］一［＃「一」は中見出し］
    
    　ある日の事でございます。御釈迦様《おしゃかさま》は極楽の蓮池《はすいけ》のふちを、独りでぶらぶら御歩きになっていらっしゃいました。池の中に咲いている蓮《はす》の花は、みんな玉のようにまっ白で、そのまん中にある金色《きんいろ》の蕊《ずい》からは、何とも云えない好《よ》い匂《におい》が、絶間《たえま》なくあたりへ溢《あふ》れて居ります。極楽は丁度朝なのでございましょう。
    　やがて御釈迦様はその池のふちに御佇《おたたず》みになって、水の面《おもて》を蔽《おお》っている蓮の葉の間から、ふと下の容子《ようす》を御覧になりました。この極楽の蓮池の下は、丁度｜地獄《じごく》の底に当って居りますから、水晶《すいしよう》のような水を透き徹して、三途《さんず》の河や針の山の景色が、丁度｜覗《のぞ》き眼鏡《めがね》を見るように、はっきりと見えるのでございます。
    　するとその地獄の底に、※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多《かんだた》と云う男が一人、ほかの罪人と一しょに蠢《うごめ》いている姿が、御眼に止まりました。この※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多と云う男は、人を殺したり家に火をつけたり、いろいろ悪事を働いた大泥坊でございますが、それでもたった一つ、善い事を致した覚えがございます。と申しますのは、ある時この男が深い林の中を通りますと、小さな蜘蛛《くも》が一匹、路ばたを這《は》って行くのが見えました。そこで※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多は早速足を挙げて、踏み殺そうと致しましたが、「いや、いや、これも小さいながら、命のあるものに違いない。その命を無暗《むやみ》にとると云う事は、いくら何でも可哀そうだ。」と、こう急に思い返して、とうとうその蜘蛛を殺さずに助けてやったからでございます。
    　御釈迦様は地獄の容子を御覧になりながら、この※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多には蜘蛛を助けた事があるのを御思い出しになりました。そうしてそれだけの善い事をした報《むくい》には、出来るなら、この男を地獄から救い出してやろうと御考えになりました。幸い、側を見ますと、翡翠《ひすい》のような色をした蓮の葉の上に、極楽の蜘蛛が一匹、美しい銀色の糸をかけて居ります。御釈迦様はその蜘蛛の糸をそっと御手に御取りになって、玉のような白蓮《しらはす》の間から、遥か下にある地獄の底へ、まっすぐにそれを御｜下《おろ》しなさいました。
    
    ［＃８字下げ］二［＃「二」は中見出し］
    
    　こちらは地獄の底の血の池で、ほかの罪人と一しょに、浮いたり沈んだりしていた※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多《かんだた》でございます。何しろどちらを見ても、まっ暗で、たまにそのくら暗からぼんやり浮き上っているものがあると思いますと、それは恐しい針の山の針が光るのでございますから、その心細さと云ったらございません。その上あたりは墓の中のようにしんと静まり返って、たまに聞えるものと云っては、ただ罪人がつく微《かすか》な嘆息《たんそく》ばかりでございます。これはここへ落ちて来るほどの人間は、もうさまざまな地獄の責苦《せめく》に疲れはてて、泣声を出す力さえなくなっているのでございましょう。ですからさすが大泥坊の※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多も、やはり血の池の血に咽《むせ》びながら、まるで死にかかった蛙《かわず》のように、ただもがいてばかり居りました。
    　ところがある時の事でございます。何気《なにげ》なく※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多が頭を挙げて、血の池の空を眺めますと、そのひっそりとした暗の中を、遠い遠い天上から、銀色の蜘蛛《くも》の糸が、まるで人目にかかるのを恐れるように、一すじ細く光りながら、するすると自分の上へ垂れて参るのではございませんか。※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多はこれを見ると、思わず手を拍《う》って喜びました。この糸に縋《すが》りついて、どこまでものぼって行けば、きっと地獄からぬけ出せるのに相違ございません。いや、うまく行くと、極楽へはいる事さえも出来ましょう。そうすれば、もう針の山へ追い上げられる事もなくなれば、血の池に沈められる事もある筈はございません。
    　こう思いましたから※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多《かんだた》は、早速その蜘蛛の糸を両手でしっかりとつかみながら、一生懸命に上へ上へとたぐりのぼり始めました。元より大泥坊の事でございますから、こう云う事には昔から、慣れ切っているのでございます。
    　しかし地獄と極楽との間は、何万里となくございますから、いくら焦《あせ》って見た所で、容易に上へは出られません。ややしばらくのぼる中《うち》に、とうとう※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多もくたびれて、もう一たぐりも上の方へはのぼれなくなってしまいました。そこで仕方がございませんから、まず一休み休むつもりで、糸の中途にぶら下りながら、遥かに目の下を見下しました。
    　すると、一生懸命にのぼった甲斐があって、さっきまで自分がいた血の池は、今ではもう暗の底にいつの間にかかくれて居ります。それからあのぼんやり光っている恐しい針の山も、足の下になってしまいました。この分でのぼって行けば、地獄からぬけ出すのも、存外わけがないかも知れません。※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多は両手を蜘蛛の糸にからみながら、ここへ来てから何年にも出した事のない声で、「しめた。しめた。」と笑いました。ところがふと気がつきますと、蜘蛛の糸の下の方には、数限《かずかぎり》もない罪人たちが、自分ののぼった後をつけて、まるで蟻《あり》の行列のように、やはり上へ上へ一心によじのぼって来るではございませんか。※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多はこれを見ると、驚いたのと恐しいのとで、しばらくはただ、莫迦《ばか》のように大きな口を開《あ》いたまま、眼ばかり動かして居りました。自分一人でさえ断《き》れそうな、この細い蜘蛛の糸が、どうしてあれだけの人数《にんず》の重みに堪える事が出来ましょう。もし万一途中で断《き》れたと致しましたら、折角ここへまでのぼって来たこの肝腎《かんじん》な自分までも、元の地獄へ逆落《さかおと》しに落ちてしまわなければなりません。そんな事があったら、大変でございます。が、そう云う中にも、罪人たちは何百となく何千となく、まっ暗な血の池の底から、うようよと這《は》い上って、細く光っている蜘蛛の糸を、一列になりながら、せっせとのぼって参ります。今の中にどうかしなければ、糸はまん中から二つに断れて、落ちてしまうのに違いありません。
    　そこで※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多は大きな声を出して、「こら、罪人ども。この蜘蛛の糸は己《おれ》のものだぞ。お前たちは一体誰に尋《き》いて、のぼって来た。下りろ。下りろ。」と喚《わめ》きました。
    　その途端でございます。今まで何ともなかった蜘蛛の糸が、急に※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多のぶら下っている所から、ぷつりと音を立てて断《き》れました。ですから※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多もたまりません。あっと云う間《ま》もなく風を切って、独楽《こま》のようにくるくるまわりながら、見る見る中に暗の底へ、まっさかさまに落ちてしまいました。
    　後にはただ極楽の蜘蛛の糸が、きらきらと細く光りながら、月も星もない空の中途に、短く垂れているばかりでございます。
    
    ［＃８字下げ］三［＃「三」は中見出し］
    
    　御釈迦様《おしゃかさま》は極楽の蓮池《はすいけ》のふちに立って、この一部｜始終《しじゅう》をじっと見ていらっしゃいましたが、やがて※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多《かんだた》が血の池の底へ石のように沈んでしまいますと、悲しそうな御顔をなさりながら、またぶらぶら御歩きになり始めました。自分ばかり地獄からぬけ出そうとする、※［＃「特のへん＋廴＋聿」、第3水準1-87-71］陀多の無慈悲な心が、そうしてその心相当な罰をうけて、元の地獄へ落ちてしまったのが、御釈迦様の御目から見ると、浅間しく思召されたのでございましょう。
    　しかし極楽の蓮池の蓮は、少しもそんな事には頓着《とんじゃく》致しません。その玉のような白い花は、御釈迦様の御足《おみあし》のまわりに、ゆらゆら萼《うてな》を動かして、そのまん中にある金色の蕊《ずい》からは、何とも云えない好《よ》い匂が、絶間《たえま》なくあたりへ溢《あふ》れて居ります。極楽ももう午《ひる》に近くなったのでございましょう。
    ［＃地から１字上げ］（大正七年四月十六日）
    
    
    
    底本：「芥川龍之介全集2」ちくま文庫、筑摩書房
    　　　1986（昭和61）年10月28日第1刷発行
    　　　1996（平成8）年7月15日第11刷発行
    親本：筑摩全集類聚版芥川龍之介全集
    　　　1971（昭和46）年3月〜11月
    入力：平山誠、野口英司
    校正：もりみつじゅんじ
    1997年11月10日公開
    2011年1月28日修正
    青空文庫作成ファイル：
    このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。
    


このままでは邪魔な部分があるので、チョチョイと整形してやりましょう。

このように文章の整形をすることも広い意味では自然言語処理になります。

しかし、今回の本筋ではないので省略します。


```
!pip install beautifulsoup4
```

    Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)


青空文庫は上の例のように本文の他にもテキストがあるのでその部分を削除します。
別に.txtファイルを用意された場合には該当箇所を削除してください


```
from bs4 import BeautifulSoup

def preprocess_for_aozora(novel):
    novel = BeautifulSoup(novel).text#html関連のタグを除去します。
    novel = novel.replace("-","")# "-"を除去します。
    novel = novel.split() #改行などの特定の記号で分割します。
    #本文を抜き出します。
    extracted_novel = []
    length_novel = len(novel )
    for num_line in range(length_novel):
        if num_line in range(2, 14):
            #前半の説明を飛ばします。
            continue
        else:
            extracted_novel.append(novel[num_line])

    extracted_novel = extracted_novel[0:-12]#著者などの情報を除去します。   
    return extracted_novel
```


    ---------------------------------------------------------------------------

    ModuleNotFoundError                       Traceback (most recent call last)

    <ipython-input-1-564d3873f07e> in <module>()
    ----> 1 from BeautifulSoup import BeautifulSoup
          2 
          3 def preprocess_for_aozora(novel):
          4     novel = BeautifulSoup(novel).text
          5     novel = novel.replace("-","")


    ModuleNotFoundError: No module named 'BeautifulSoup'

    

    ---------------------------------------------------------------------------
    NOTE: If your import is failing due to a missing package, you can
    manually install dependencies using either !pip or !apt.
    
    To view examples of installing some common dependencies, click the
    "Open Examples" button below.
    ---------------------------------------------------------------------------




```
processed_novel =  preprocess_for_aozora(novel)
```


```
line = extract_words(processed_novel[0])
for word in line:
    print(word)
```

    蜘蛛
    糸


### 単語の処理
では次に文章から全ての単語のリストとボキャブラリー、
さらに単語に番号を割り振ります。
これは単語のままでは計算機が扱いに困るためにひとまずは番号で区別してやるということです。


```
"""
テキストから単語を抜き出して一連のリストを作ります。
word_list=[[word1, word2, ..., wordn],[],[] ]
"""
word_list=[]
for sentence in processed_novel:
    for text in sentence.split("。"):
        word_list.append(extract_words(text))


# 
# w2v start
"""
voabularyは重複無しの単語のリストです。
list(set(word_list))でもいいかも。
"""
vocabulary = []
for sentence in word_list:
    for word in sentence:
        if word not in vocabulary:
            vocabulary.append(word)

"""
単語に番号を割り振って、相互に変換できる辞書を作成します。
"""
word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}
idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}
vocabulary_size = len(vocabulary)
```

では次にそれぞれの単語の周りにはどんな単語があったのかを抜き出していきます。

この時、window_sizeというのは前後どこまでが関係する単語か、という範囲を指定しています。

non-related<-related words<- target word->related words->non-related

適宜、調整して実験してみてください。


```
"""
window_sizeは前後何単語までが関係があるとみなすかのパラメータです。
idx_pairsはwindow_sizeの範囲内の単語のペアを番号で表現しています。

"""
window_size = 5

idx_pairs = []
for sentence in word_list:
    indices = [word2idx[word] for word in sentence]#文中の単語をその順番を壊さずに番号に置き換えます。
    for center_word_pos in range(len(indices)):#焦点を当てる単語の位置を決めます。
        for w in range(-window_size, window_size+1):#焦点を当てた単語から前後何単語までを学習するかの範囲を指定しています。
            context_word_pos = center_word_pos+w#学習のために指定する単語の位置を指定します。
            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:
                #指定した単語の位置が範囲外の場合、また焦点を当てている単語と重複した場合をスキップします。
                continue
            context_word_idx = indices[context_word_pos]#指定した単語の番号を取得します。
            idx_pairs.append((indices[center_word_pos], context_word_idx))



```

ここで得られたidx_pairsは文章中で近くに現れた単語の番号を示しています。



```
idx_pairs[0:10]
```




    [(0, 1),
     (1, 0),
     (2, 3),
     (3, 2),
     (4, 5),
     (5, 4),
     (6, 7),
     (6, 8),
     (6, 9),
     (6, 10)]




```
for (i,j) in idx_pairs[0:10]:
    print("{}:{}".format(idx2word[i], idx2word[j]))
```

### Skip-gram モデル
今回はSkip-gramという方法を使います。
これは、ある単語からその近くに現れそうな単語を予測するというモデルです。

単語のonehot表現→中間層→複数の単語を示すsoftmax

そんなことをして何の意味があるのか？と言いますと、近くに現れる単語を予測できるということは逆に捉えればそれらの単語の意味を集約している、とも言えるということです!

ならその中間層では意味のある特徴が表れているに違いない！

というのがSkip-gramの要点です。
まあ、まずは動かしてみましょう。モデルを定義します。

入力がボキャブラリーサイズのonehotベクトル、中間層が一層、出力がボキャブラリーサイズのベクトルになる単純なFFNNです。



```
import torch
import torch.nn as nn
import torch.nn.functional as F

class W2V(nn.Module):
    def __init__(self, input_size, hidden_size):
        """
        一つの単語からそれに該当する複数の単語を予測するマルチラベル問題として考えられるので、sigmoidを使います。
        今回は扱わなかったCBOWという手法では逆に複数の単語から一つの単語を予測するので、softmaxを使うと良いでしょう。
        hidden_sizeは中間層の次元であり、同時に分散表現の次元を意味します。
        
        """
        super(W2V, self).__init__()
        self.sigmoid = nn.Sigmoid()
        self.i2h = nn.Linear(input_size, hidden_size)
        self.h2o = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        h = self.i2h(x)
        return self.sigmoid(self.h2o(h), dim=0).view(1, -1)


```

では実際に学習させてみましょう！

まずは上のランタイムからランタイムのタイプを変更、GPUを選択しておいてください。学習が高速で進むようになります。

以下のコードを実行すると学習が行われます。

パラメータは色々といじって実験してみましょう。


```

import numpy as np
"""
まずはgpuを使えるかを確認します。
"""
USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")
"""
パラメータとして中間層の次元、エポック数、学習率などがあります。
意外とこれらの数値で結果が変わります。

以下ではモデルの呼び出しを行っています。
"""
embedding_dims = 20
model = W2V(vocabulary_size, embedding_dims)
if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs
    model = nn.DataParallel(model)
model.to(device)


num_epocs = 100
learning_rate = 0.001
"""
以下では変数の作成と最適化モジュールの準備をしています。
"""
idx_pairs= torch.tensor(idx_pairs)
idx_pairs=idx_pairs.to(device)

optimizer = torch.optim.Adam(model.parameters())

"""
以下がトレーニングのステップになります。
"""
for epoch in range(num_epocs):
    loss = 0
    for data, target in idx_pairs:
        x = torch.eye(vocabulary_size)[data].to(device)#単語の番号をonehot表現のベクトルに変換しています。
        y_true = target.view(1)
        y_pred = model(x)#モデルを用いて共起する単語の予測をしています。
        loss += F.nll_loss(y_pred, y_true)#損失を計算しています。

    """
    以下で逆伝搬、最適化の処理を行っています。
    """
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print("epoch:{}, loss:{}".format(epoch, loss.item()))

```


    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    <ipython-input-15-4f5ed8c6f78c> in <module>()
         27         x = torch.eye(vocabulary_size)[data].to(device)
         28         y_true = target.view(1)
    ---> 29         y_pred = model(x)
         30         loss += F.nll_loss(y_pred, y_true)
         31 


    /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
        491             result = self._slow_forward(*input, **kwargs)
        492         else:
    --> 493             result = self.forward(*input, **kwargs)
        494         for hook in self._forward_hooks.values():
        495             hook_result = hook(self, input, result)


    <ipython-input-14-de9453d4f4f8> in forward(self, x)
         12     def forward(self, x):
         13         h = self.i2h(x)
    ---> 14         return self.sigmoid(self.h2o(h), dim=0).view(1, -1)
         15 


    /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)
        537                 return modules[name]
        538         raise AttributeError("'{}' object has no attribute '{}'".format(
    --> 539             type(self).__name__, name))
        540 
        541     def __setattr__(self, name, value):


    AttributeError: 'W2V' object has no attribute 'sigmoid'


### 分散表現の確認
では学習の結果を確認してみましょう。

以下のブロックを実行してみてください。




```
sample_text = "例えばこんな文章があったとして、どうなるでしょうか？"

sample_words = extract_words(sample_text)#文から単語を抜き出す
for word in sample_words:
    print(word)
    idx = word2idx.get(word, 10)
    x = torch.eye(vocabulary_size)[idx].to(device)#単語の番号からonehotベクトルを作成する
    print(model.i2h(x))#モデルから中間表現を抜き出す。
```

単語とそれに対応するベクトルが得られました！

これは意味空間における単語のベクトルを学習したということになります。

ここまでくればあとは応用です。
ベクトルでできることは何でもできるようになります。
単語同士のベクトルで四則演算をしたり、cos類似度から検索をしたり、要約をしたりできます。

例えば以下の単語のcos類似度を見てみましょう。
これはベクトルがどれほど似ているかを示す指標になります。


```
sample_word1 = "地獄"
sample_word2 ="血"
sample_word3 = "菩薩"
sample_words = [sample_word1,sample_word2,sample_word3]
embedding_vectors = []
for word in sample_words:
    idx = word2idx.get(word, 10)
    x = torch.eye(vocabulary_size)[idx].to(device)
    embedding_vectors.append(model.i2h(x))
```


```
cos = nn.CosineSimilarity(dim=1, eps=1e-6)
for ind, embedding_vector in enumerate(embedding_vectors):
    next_ind = ind+1
    if  next_ind==len(embedding_vectors):
        next_ind = 0
    print("{}と{}".format(sample_words[ind], sample_words[next_ind]))
    print(cos(embedding_vector.view(1,-1), embedding_vectors[next_ind].view(1,-1)))


```

想像通りの結果が得られているでしょうか？
それとも意外な結果になっているでしょうか？
直感的には地獄と血はとても近いイメージですが、
地獄と菩薩の意味が近いみたいですね。
まさに地獄に仏・・・。

### Bag Of Words(BOW)
ここまでで単語をベクトルにすることができました。
しかし、文章をベクトルにするにはどうすればいいでしょうか？
一つの解決案は単語ベクトルを足し合わせる(Bag Of Words)という方法です。直訳するとカバンの中の単語ということでごちゃ混ぜになって、順番などは考慮していないということですね。
実際に例を見てみましょう。



```
texts_embed={}

def embed_text(text):
    #print(text)
    text_embed = np.zeros(embedding_dims)#文のベクトルを準備する
    words = extract_words(text)
    for word in words:
        #print(word)
        idx = word2idx.get(word, 10)
        x = torch.eye(vocabulary_size)[idx]
        word_embed=model.i2h(x.to(device)).cpu().detach().numpy()
        #print(word_embed)
        text_embed=text_embed+word_embed#抜き出した単語の分散表現を足し合わせる
    return text_embed

for sentence in processed_novel:
    for text in sentence.split("。"):
        text_embed=embed_text(text)
        texts_embed[text]=text_embed
        
del texts_embed[""]#空白は除去
```


```
for k,v  in texts_embed.items():
    print("{}:{}".format(k,v))
```


```

for ind, (k,v)  in enumerate(texts_embed.items()):
    if ind ==0:
        texts_list = [k]
        embed_arrray=v
    else:
        texts_list.append(k)
        embed_arrray=np.vstack([embed_arrray, v])
```

このようにそれぞれの文に対応するベクトルが得られました。

## 応用
以上で単語や文に対してベクトルで意味を表現することができました。
応用としてはニュース記事の内容からジャンルへの振り分け、意味の近い文の検索などが可能です。
さらにそこから踏み込んで、文書要約、機械翻訳、といったものが実現します。
今回は以下の二つについて、実際にコードを動かしてみましょう。
- クラスタリング
- 検索

### クラスタリング
では文書ベクトルを用いて、クラスタリングをしてみましょう。

今回はクラスタリングは


```
import matplotlib.pyplot as plt

from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


n_digits=5
reduced_data = PCA(n_components=2).fit_transform(   embed_arrray  )
kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)
kmeans.fit(reduced_data)

# Step size of the mesh. Decrease to increase the quality of the VQ.
h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].

# Plot the decision boundary. For that, we will assign a color to each
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Obtain labels for each point in mesh. Use last trained model.
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1)
plt.clf()
plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap=plt.cm.Paired,
           aspect='auto', origin='lower')

plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)
# Plot the centroids as a white X
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1],
            marker='x', s=169, linewidths=3,
            color='w', zorder=10)
plt.title('K-means clustering on the text embedding dataset (PCA-reduced data)\n'
          'Centroids are marked with white cross')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
plt.show()
```

### 検索

同様に文書ベクトルを用いて、検索をしてみましょう。
queryに好きな文を入力してみてください。
その文のベクトルに類似したベクトルを持つ文が順番に表示されます。


```
from scipy import spatial

query = "地獄に仏とはまさにこのこと"
query_embed = embed_text(query)

sim_list = []

for ind, (k,v)  in enumerate(texts_embed.items()):
    result = 1 - spatial.distance.cosine(query_embed, v)
    sim_list.append(result)

sim_array=np.asarray(sim_list)
descending_order= np.argsort(sim_array, axis=0)[::-1]
for i in range(5):
    ind = descending_order[i]
    print("text:{}, similarity:{}".format(texts_list[ind], sim_array[ind]))
    


```

## その他の分散表現

以上でW2Vについて触れてもらいました。

W2Vは単語を任意の固定長ベクトルで表現できるという点で、非常に汎用性が高く、また表現力も高く演算に耐えうるという点で非常に強力なモデルになっています。

このように、実際の用途に用いる前に学習を済ませてあるモデルのことを事前学習モデル(pre-trained model)と言います。これは非常に大きな役割を担っており、W2V以降もいくつものモデルが提案されてきました。
分散表現が発展するに従って、その後のタスクの性能が顕著に向上するという例がいくつもみられてきました。

その中でもとりわけ強力なのが**BERT**と呼ばれています。
これはやはり事前学習モデルの一つですが、これによって多くのタスクにおいてSTAが更新されました。

このことからも単語や文の意味を表現することの重要性がわかります。

このように計算機に文章の意味を理解させるという取り組みは活発に行われており、それに伴って分散表現もいくつも発明されています。

そしてその中で有名なものは以下のライブラリ**flair**を活用することによって簡単に利用することができます。

是非とも遊んでみてください。


```
!pip install flair
import flair
from flair.data import Sentence
## Importing the Embeddings ##
from flair.embeddings import WordEmbeddings
from flair.embeddings import CharacterEmbeddings
from flair.embeddings import StackedEmbeddings
from flair.embeddings import FlairEmbeddings
from flair.embeddings import BertEmbeddings
from flair.embeddings import  DocumentPoolEmbeddings
```

ここではfasttextという手法を利用しています。
他の手法の利用法などはhttps://github.com/zalandoresearch/flair
を参照してください。

コツを申し上げると、スペース区切りの文に対して分散表現の埋め込みを行ってくれるので、その点には注意が必要です。


```
fasttext_embedding = WordEmbeddings('ja')

```


```
def make_tokenized_by_blank(text):
    tokenized_text=extract_words(text)
    tokenized_text=" ".join(tokenized_text)
    return tokenized_text
    

def embed_to_token(tokens, embedding=fasttext_embedding):
    sentence = tokens
    sentence = Sentence(sentence)
    embedding.embed(sentence)  
    
    query_vector = [token.embedding.clone().detach()for token in sentence]
    return query_vector
```


```
sample_text2 = sample_word1+sample_word2+sample_word3
sample_tokens = make_tokenized_by_blank(sample_text2)
embedding_vectors2 = embed_to_token(sample_tokens)
sample_tokens2 = sample_tokens.split(" ")

for ind, embedding_vector in enumerate(embedding_vectors2):
    next_ind = ind+1
    if  next_ind==len(embedding_vectors):
        next_ind = 0
        
    print("{}と{}".format(sample_tokens2[ind], sample_tokens2[next_ind]))

    print(cos(embedding_vector.view(1,-1), embedding_vectors2[next_ind].view(1,-1)))



```

こちらのモデルだと地獄と血の意味が近いみたいですね。
これは学習にWikipediaを用いているため、より一般的な意味が学習されたのでしょう。


## まとめ
以上です。
いかがだったでしょうか。
２０１９年現在も自然言語処理は研究が盛んなトピックの一つです。
しかし、DNNはまだ手探りでモデル設計がなされている状況が続いています。
いずれは効率的なモデル設計やその構造の意味の解析も進んでくるでしょう。
しかし、今はまだベンチマークの結果で性能を評価するだけの状態です。
BERTが一世を風靡し、その改良版が出現し、これから先の発展に期待が持てます。


## 参考図書
- 自然言語処理の基本と技術 
グラム・ニュービッグ  (著), 萩原正人 (著), 奥野陽 (編集), 小町守 (監修
- 吾輩は猫である, 夏目漱石
- 蜘蛛の糸, 芥川龍之介
- Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems. 2013.

- Akbik, Alan, Duncan Blythe, and Roland Vollgraf. "Contextual string embeddings for sequence labeling." Proceedings of the 27th International Conference on Computational Linguistics. 2018.

-  Janome : Japanese morphological analysis engine written in Python

- Scikit-learn: Machine Learning in Python, Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay; 12(Oct):2825−2830, 2011.
